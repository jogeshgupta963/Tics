* 
* ==> Audit <==
* |------------|--------------------------------|----------|----------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |   User   | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|----------|---------|---------------------|---------------------|
| start      |                                | minikube | jogesh   | v1.28.0 | 03 Dec 22 00:52 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 03 Dec 22 01:01 IST |                     |
| ip         |                                | minikube | jogesh   | v1.28.0 | 05 Dec 22 21:00 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 05 Dec 22 21:01 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 05 Dec 22 21:01 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 05 Dec 22 21:05 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 07 Dec 22 22:26 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 08 Dec 22 09:50 IST | 08 Dec 22 09:54 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 10:07 IST | 08 Dec 22 10:07 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 10:10 IST | 08 Dec 22 10:10 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| image      | load mongo                     | minikube | jogesh   | v1.28.0 | 08 Dec 22 10:15 IST | 08 Dec 22 10:16 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 10:16 IST | 08 Dec 22 10:16 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 21:23 IST |                     |
|            | --user=skaffold                |          |          |         |                     |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 08 Dec 22 21:24 IST |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 08 Dec 22 21:24 IST | 08 Dec 22 21:24 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 21:27 IST | 08 Dec 22 21:27 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:02 IST | 08 Dec 22 22:02 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:05 IST | 08 Dec 22 22:05 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:07 IST | 08 Dec 22 22:07 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:09 IST | 08 Dec 22 22:09 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:12 IST | 08 Dec 22 22:12 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 08 Dec 22 22:14 IST | 08 Dec 22 22:14 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 09 Dec 22 01:32 IST | 09 Dec 22 01:32 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 09 Dec 22 01:34 IST | 09 Dec 22 01:34 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 09 Dec 22 01:38 IST | 09 Dec 22 01:38 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| ip         |                                | minikube | jogesh   | v1.28.0 | 09 Dec 22 01:41 IST | 09 Dec 22 01:41 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 09 Dec 22 02:20 IST | 09 Dec 22 02:20 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 09 Dec 22 02:30 IST | 09 Dec 22 02:30 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| start      |                                | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:34 IST | 09 Dec 22 02:34 IST |
| docker-env | --shell none -p minikube       | minikube | skaffold | v1.28.0 | 09 Dec 22 02:35 IST | 09 Dec 22 02:35 IST |
|            | --user=skaffold                |          |          |         |                     |                     |
| addons     | enable ingress                 | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:45 IST |                     |
| addons     | enable ingress                 | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:47 IST |                     |
| addons     | enable ingress                 | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:49 IST |                     |
| addons     | disable ingress                | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:51 IST | 09 Dec 22 02:51 IST |
| addons     | enable ingress                 | minikube | jogesh   | v1.28.0 | 09 Dec 22 02:51 IST |                     |
|------------|--------------------------------|----------|----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/12/09 02:34:28
Running on machine: jogesh
Binary: Built with gc go1.19.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1209 02:34:28.553269    2791 out.go:296] Setting OutFile to fd 1 ...
I1209 02:34:28.553500    2791 out.go:348] isatty.IsTerminal(1) = true
I1209 02:34:28.553505    2791 out.go:309] Setting ErrFile to fd 2...
I1209 02:34:28.553511    2791 out.go:348] isatty.IsTerminal(2) = true
I1209 02:34:28.553564    2791 root.go:334] Updating PATH: /home/jogesh/.minikube/bin
W1209 02:34:28.553866    2791 root.go:311] Error reading config file at /home/jogesh/.minikube/config/config.json: open /home/jogesh/.minikube/config/config.json: no such file or directory
I1209 02:34:28.554371    2791 out.go:303] Setting JSON to false
I1209 02:34:28.555969    2791 start.go:116] hostinfo: {"hostname":"jogesh","uptime":193,"bootTime":1670533275,"procs":314,"os":"linux","platform":"arch","platformFamily":"arch","platformVersion":"\"rolling\"","kernelVersion":"6.0.10-arch2-1","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"5fa293d2-e0b4-423a-8239-dac5369e3c84"}
I1209 02:34:28.556002    2791 start.go:126] virtualization: kvm host
I1209 02:34:28.556927    2791 out.go:177] 😄  minikube v1.28.0 on Arch "rolling"
I1209 02:34:28.557630    2791 notify.go:220] Checking for updates...
I1209 02:34:28.558027    2791 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1209 02:34:28.558624    2791 driver.go:365] Setting default libvirt URI to qemu:///system
I1209 02:34:28.609864    2791 docker.go:137] docker version: linux-20.10.21
I1209 02:34:28.609919    2791 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1209 02:34:28.625774    2791 info.go:266] docker info: {ID:6IEL:UNUA:76PU:QMR6:XUZV:ERZQ:JDLZ:IUG3:ESAF:QFTH:VGSD:WGCE Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:26 OomKillDisable:false NGoroutines:43 SystemTime:2022-12-09 02:34:28.62016068 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.0.10-arch2-1 OperatingSystem:EndeavourOS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16072589312 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jogesh Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661.m Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I1209 02:34:28.625819    2791 docker.go:254] overlay module found
I1209 02:34:28.626443    2791 out.go:177] ✨  Using the docker driver based on existing profile
I1209 02:34:28.626923    2791 start.go:282] selected driver: docker
I1209 02:34:28.626932    2791 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/jogesh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1209 02:34:28.626992    2791 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1209 02:34:28.627032    2791 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1209 02:34:28.642109    2791 info.go:266] docker info: {ID:6IEL:UNUA:76PU:QMR6:XUZV:ERZQ:JDLZ:IUG3:ESAF:QFTH:VGSD:WGCE Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:26 OomKillDisable:false NGoroutines:43 SystemTime:2022-12-09 02:34:28.636598665 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.0.10-arch2-1 OperatingSystem:EndeavourOS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16072589312 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jogesh Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661.m Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I1209 02:34:28.643011    2791 cni.go:95] Creating CNI manager for ""
I1209 02:34:28.643019    2791 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1209 02:34:28.643151    2791 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/jogesh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1209 02:34:28.644252    2791 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1209 02:34:28.644727    2791 cache.go:120] Beginning downloading kic base image for docker with docker
I1209 02:34:28.645200    2791 out.go:177] 🚜  Pulling base image ...
I1209 02:34:28.645670    2791 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1209 02:34:28.645701    2791 preload.go:148] Found local preload: /home/jogesh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I1209 02:34:28.645707    2791 cache.go:57] Caching tarball of preloaded images
I1209 02:34:28.645746    2791 image.go:76] Checking for gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon
I1209 02:34:28.645837    2791 preload.go:174] Found /home/jogesh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1209 02:34:28.645846    2791 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.3 on docker
I1209 02:34:28.645945    2791 profile.go:148] Saving config to /home/jogesh/.minikube/profiles/minikube/config.json ...
I1209 02:34:28.659275    2791 image.go:80] Found gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 in local docker daemon, skipping pull
I1209 02:34:28.659290    2791 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 exists in daemon, skipping load
I1209 02:34:28.659303    2791 cache.go:208] Successfully downloaded all kic artifacts
I1209 02:34:28.659332    2791 start.go:364] acquiring machines lock for minikube: {Name:mkb9eb1603602d36dc380ce1cfe77b398d27a6a8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1209 02:34:28.659416    2791 start.go:368] acquired machines lock for "minikube" in 63.905µs
I1209 02:34:28.659431    2791 start.go:96] Skipping create...Using existing machine configuration
I1209 02:34:28.659439    2791 fix.go:55] fixHost starting: 
I1209 02:34:28.659626    2791 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 02:34:28.678359    2791 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1209 02:34:28.678373    2791 fix.go:129] unexpected machine state, will restart: <nil>
I1209 02:34:28.679572    2791 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1209 02:34:28.680066    2791 cli_runner.go:164] Run: docker start minikube
I1209 02:34:29.095108    2791 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 02:34:29.106186    2791 kic.go:415] container "minikube" state is running.
I1209 02:34:29.106419    2791 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 02:34:29.118680    2791 profile.go:148] Saving config to /home/jogesh/.minikube/profiles/minikube/config.json ...
I1209 02:34:29.118797    2791 machine.go:88] provisioning docker machine ...
I1209 02:34:29.118814    2791 ubuntu.go:169] provisioning hostname "minikube"
I1209 02:34:29.118845    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:29.130905    2791 main.go:134] libmachine: Using SSH client type: native
I1209 02:34:29.131099    2791 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1209 02:34:29.131108    2791 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1209 02:34:29.131484    2791 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52274->127.0.0.1:49157: read: connection reset by peer
I1209 02:34:32.263633    2791 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1209 02:34:32.263714    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.275716    2791 main.go:134] libmachine: Using SSH client type: native
I1209 02:34:32.275800    2791 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1209 02:34:32.275810    2791 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1209 02:34:32.375630    2791 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1209 02:34:32.375684    2791 ubuntu.go:175] set auth options {CertDir:/home/jogesh/.minikube CaCertPath:/home/jogesh/.minikube/certs/ca.pem CaPrivateKeyPath:/home/jogesh/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/jogesh/.minikube/machines/server.pem ServerKeyPath:/home/jogesh/.minikube/machines/server-key.pem ClientKeyPath:/home/jogesh/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/jogesh/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/jogesh/.minikube}
I1209 02:34:32.375704    2791 ubuntu.go:177] setting up certificates
I1209 02:34:32.375717    2791 provision.go:83] configureAuth start
I1209 02:34:32.375755    2791 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 02:34:32.386479    2791 provision.go:138] copyHostCerts
I1209 02:34:32.386875    2791 exec_runner.go:144] found /home/jogesh/.minikube/ca.pem, removing ...
I1209 02:34:32.386884    2791 exec_runner.go:207] rm: /home/jogesh/.minikube/ca.pem
I1209 02:34:32.386918    2791 exec_runner.go:151] cp: /home/jogesh/.minikube/certs/ca.pem --> /home/jogesh/.minikube/ca.pem (1078 bytes)
I1209 02:34:32.387338    2791 exec_runner.go:144] found /home/jogesh/.minikube/cert.pem, removing ...
I1209 02:34:32.387343    2791 exec_runner.go:207] rm: /home/jogesh/.minikube/cert.pem
I1209 02:34:32.387357    2791 exec_runner.go:151] cp: /home/jogesh/.minikube/certs/cert.pem --> /home/jogesh/.minikube/cert.pem (1123 bytes)
I1209 02:34:32.387462    2791 exec_runner.go:144] found /home/jogesh/.minikube/key.pem, removing ...
I1209 02:34:32.387467    2791 exec_runner.go:207] rm: /home/jogesh/.minikube/key.pem
I1209 02:34:32.387481    2791 exec_runner.go:151] cp: /home/jogesh/.minikube/certs/key.pem --> /home/jogesh/.minikube/key.pem (1675 bytes)
I1209 02:34:32.387573    2791 provision.go:112] generating server cert: /home/jogesh/.minikube/machines/server.pem ca-key=/home/jogesh/.minikube/certs/ca.pem private-key=/home/jogesh/.minikube/certs/ca-key.pem org=jogesh.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1209 02:34:32.486813    2791 provision.go:172] copyRemoteCerts
I1209 02:34:32.486850    2791 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1209 02:34:32.486890    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.498291    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:32.573290    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1209 02:34:32.583569    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1209 02:34:32.592631    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1209 02:34:32.601397    2791 provision.go:86] duration metric: configureAuth took 225.668788ms
I1209 02:34:32.601408    2791 ubuntu.go:193] setting minikube options for container-runtime
I1209 02:34:32.601531    2791 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1209 02:34:32.601563    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.612685    2791 main.go:134] libmachine: Using SSH client type: native
I1209 02:34:32.612767    2791 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1209 02:34:32.612774    2791 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1209 02:34:32.724133    2791 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1209 02:34:32.724150    2791 ubuntu.go:71] root file system type: overlay
I1209 02:34:32.724407    2791 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1209 02:34:32.724449    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.735867    2791 main.go:134] libmachine: Using SSH client type: native
I1209 02:34:32.735947    2791 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1209 02:34:32.735987    2791 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1209 02:34:32.843060    2791 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1209 02:34:32.843103    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.854313    2791 main.go:134] libmachine: Using SSH client type: native
I1209 02:34:32.854403    2791 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ed4e0] 0x7f0660 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1209 02:34:32.854417    2791 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1209 02:34:32.970828    2791 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1209 02:34:32.970844    2791 machine.go:91] provisioned docker machine in 3.852036467s
I1209 02:34:32.970859    2791 start.go:300] post-start starting for "minikube" (driver="docker")
I1209 02:34:32.970869    2791 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1209 02:34:32.970912    2791 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1209 02:34:32.970953    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:32.982643    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:33.058411    2791 ssh_runner.go:195] Run: cat /etc/os-release
I1209 02:34:33.059813    2791 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1209 02:34:33.059828    2791 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1209 02:34:33.059836    2791 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1209 02:34:33.059841    2791 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1209 02:34:33.059848    2791 filesync.go:126] Scanning /home/jogesh/.minikube/addons for local assets ...
I1209 02:34:33.060067    2791 filesync.go:126] Scanning /home/jogesh/.minikube/files for local assets ...
I1209 02:34:33.060169    2791 start.go:303] post-start completed in 89.301233ms
I1209 02:34:33.060191    2791 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1209 02:34:33.060221    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:33.071853    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:33.146042    2791 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1209 02:34:33.147972    2791 fix.go:57] fixHost completed within 4.48853251s
I1209 02:34:33.147985    2791 start.go:83] releasing machines lock for "minikube", held for 4.488558701s
I1209 02:34:33.148028    2791 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1209 02:34:33.159484    2791 ssh_runner.go:195] Run: systemctl --version
I1209 02:34:33.159520    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:33.159547    2791 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1209 02:34:33.159589    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:33.171903    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:33.172124    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:33.656812    2791 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1209 02:34:33.663158    2791 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1209 02:34:33.663184    2791 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1209 02:34:33.669321    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1209 02:34:33.676135    2791 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1209 02:34:33.782424    2791 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1209 02:34:33.826694    2791 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 02:34:33.874094    2791 ssh_runner.go:195] Run: sudo systemctl restart docker
I1209 02:34:34.787530    2791 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1209 02:34:34.863319    2791 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1209 02:34:34.909595    2791 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1209 02:34:34.915303    2791 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1209 02:34:34.915378    2791 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1209 02:34:34.916986    2791 start.go:472] Will wait 60s for crictl version
I1209 02:34:34.917008    2791 ssh_runner.go:195] Run: sudo crictl version
I1209 02:34:35.113100    2791 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I1209 02:34:35.113132    2791 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 02:34:35.160635    2791 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1209 02:34:35.176749    2791 out.go:204] 🐳  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I1209 02:34:35.177155    2791 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1209 02:34:35.188598    2791 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1209 02:34:35.190442    2791 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 02:34:35.195357    2791 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1209 02:34:35.195383    2791 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 02:34:35.214594    2791 docker.go:613] Got preloaded images: -- stdout --
jogeshgupta963/auth:3e9aa95facb54a13f9dd356c77948020292dae1f9b4eab8a207817cf7ea38773
jogeshgupta963/auth:c7c665d-dirty
jogeshgupta963/tickets:631900e49df4af3ec264c7f09eaf48eae73730224c78fd6e49b86d9d45685638
jogeshgupta963/tickets:c7c665d-dirty
jogeshgupta963/tickets:1415afea8a0b123789626c6fd680020494f6a3383e6eebe79af3322aa094ce69
jogeshgupta963/tickets:93a413b8d4babf6c88ca9535c023b537b549471ff6842cc7c5c0516f1edf851c
jogeshgupta963/tickets:cfa826a32fb35ae3c16302cc2873b6f0016161218939c9f71137deefada2ff02
jogeshgupta963/tickets:3c4b874b75ee725af56e9a92bd1af4894b54dd24161c016321c6b147ace84b89
jogeshgupta963/tickets:0c78f11dcda35fcf71af94a57464a37e6140d6c66e28c0d257b09f50eec79c84
jogeshgupta963/tickets:f5d30f50a2870b082e90cab71809b5a088c1df1a9d2171ab6044897b1c716d64
jogeshgupta963/tickets:bd9f4026f040cf759d35ff40b341ba5362826b7e6a8fbc72a2820c8a3e4c6718
jogeshgupta963/tickets:1a5c496a65c2d5f6796b13cdd794301028e72604c3ee6bb221b070274882ce09
jogeshgupta963/tickets:0de395aa23c1aebc6f61581967bb9beff560167c8dd7cac27ff402ed9de9427d
jogeshgupta963/tickets:bd1278854d87c5e8e63f74c7361f0182cd5630db53a0ff40ee1a09e56fbfbbf4
jogeshgupta963/tickets:2ac14c5d1f66c7b0171a42c6ca9191e07ebaf9f20b8e4bb53c38742d300e9229
jogeshgupta963/tickets:81cb1f5fc0ef0b60442ae80c2b8a1cfa401ecfac888c0605073c4cfe51bfb7bc
jogeshgupta963/tickets:7b49f73472ee13ede766f98b9b98cc0daf79dbc03aa679a2ac2209f7a472631b
jogeshgupta963/tickets:f579ce39f3d6e27692e7a617161a74c4ef35abafb0cb2c10cb79677f9ea83824
jogeshgupta963/tickets:b9278d86557e341db6f419384aa4b9098b234587287770ac892fb9227f2693f2
jogeshgupta963/tickets:a74214132de872bd757cdc440e82a721979c9bb67324c95a99743e4c2437ed18
jogeshgupta963/tickets:c4c9dae23b3a3eb151af8435e4e12bbf5a1de06d14174f3197fb954970dba840
jogeshgupta963/tickets:0d4feacdf9a422fce314cb566a23b994e9a0e72ef95e77384cabb154baef3711
jogeshgupta963/tickets:acb070aaa14faa09e5f89ba2fad21b01dc15f7bf2e01c795fc1972ad41fb1836
jogeshgupta963/tickets:a4ddee1cb8fed83201a68096da7d2046773306c7daa945e620188cd31f674f31
jogeshgupta963/tickets:0b9b6120b47d65cad117d53c5509b3742fa54c77848684a8954871fba608a5b6
jogeshgupta963/tickets:c7c665d
jogeshgupta963/auth:c7c665d
jogeshgupta963/auth:dbcb2db-dirty
jogeshgupta963/auth:ea8fb3c357ece7c5f381fec1d8e3cf1cd0c98c1246391c4e23ed71be87e3905f
jogeshgupta963/auth:75ab7595fa2f8afe6894ed4cb2ca9cdd33c710567cfb1dca55ac331b6fd5bf3d
jogeshgupta963/auth:b126efa-dirty
jogeshgupta963/auth:289e632f48da5a72255ae8228f66c83dc4c54d7938f2f3ce7962149771b5c40e
jogeshgupta963/auth:67c854420b61c54aca829ccd5559e0f317735cc270703f3c16f3ac3d626dc2f5
jogeshgupta963/auth:b126efa
mongo:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1209 02:34:35.214609    2791 docker.go:543] Images already preloaded, skipping extraction
I1209 02:34:35.214641    2791 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1209 02:34:35.229570    2791 docker.go:613] Got preloaded images: -- stdout --
jogeshgupta963/auth:3e9aa95facb54a13f9dd356c77948020292dae1f9b4eab8a207817cf7ea38773
jogeshgupta963/auth:c7c665d-dirty
jogeshgupta963/tickets:631900e49df4af3ec264c7f09eaf48eae73730224c78fd6e49b86d9d45685638
jogeshgupta963/tickets:c7c665d-dirty
jogeshgupta963/tickets:1415afea8a0b123789626c6fd680020494f6a3383e6eebe79af3322aa094ce69
jogeshgupta963/tickets:93a413b8d4babf6c88ca9535c023b537b549471ff6842cc7c5c0516f1edf851c
jogeshgupta963/tickets:cfa826a32fb35ae3c16302cc2873b6f0016161218939c9f71137deefada2ff02
jogeshgupta963/tickets:3c4b874b75ee725af56e9a92bd1af4894b54dd24161c016321c6b147ace84b89
jogeshgupta963/tickets:0c78f11dcda35fcf71af94a57464a37e6140d6c66e28c0d257b09f50eec79c84
jogeshgupta963/tickets:f5d30f50a2870b082e90cab71809b5a088c1df1a9d2171ab6044897b1c716d64
jogeshgupta963/tickets:bd9f4026f040cf759d35ff40b341ba5362826b7e6a8fbc72a2820c8a3e4c6718
jogeshgupta963/tickets:1a5c496a65c2d5f6796b13cdd794301028e72604c3ee6bb221b070274882ce09
jogeshgupta963/tickets:0de395aa23c1aebc6f61581967bb9beff560167c8dd7cac27ff402ed9de9427d
jogeshgupta963/tickets:bd1278854d87c5e8e63f74c7361f0182cd5630db53a0ff40ee1a09e56fbfbbf4
jogeshgupta963/tickets:2ac14c5d1f66c7b0171a42c6ca9191e07ebaf9f20b8e4bb53c38742d300e9229
jogeshgupta963/tickets:81cb1f5fc0ef0b60442ae80c2b8a1cfa401ecfac888c0605073c4cfe51bfb7bc
jogeshgupta963/tickets:7b49f73472ee13ede766f98b9b98cc0daf79dbc03aa679a2ac2209f7a472631b
jogeshgupta963/tickets:f579ce39f3d6e27692e7a617161a74c4ef35abafb0cb2c10cb79677f9ea83824
jogeshgupta963/tickets:b9278d86557e341db6f419384aa4b9098b234587287770ac892fb9227f2693f2
jogeshgupta963/tickets:a74214132de872bd757cdc440e82a721979c9bb67324c95a99743e4c2437ed18
jogeshgupta963/tickets:c4c9dae23b3a3eb151af8435e4e12bbf5a1de06d14174f3197fb954970dba840
jogeshgupta963/tickets:0d4feacdf9a422fce314cb566a23b994e9a0e72ef95e77384cabb154baef3711
jogeshgupta963/tickets:acb070aaa14faa09e5f89ba2fad21b01dc15f7bf2e01c795fc1972ad41fb1836
jogeshgupta963/tickets:a4ddee1cb8fed83201a68096da7d2046773306c7daa945e620188cd31f674f31
jogeshgupta963/tickets:0b9b6120b47d65cad117d53c5509b3742fa54c77848684a8954871fba608a5b6
jogeshgupta963/tickets:c7c665d
jogeshgupta963/auth:c7c665d
jogeshgupta963/auth:dbcb2db-dirty
jogeshgupta963/auth:ea8fb3c357ece7c5f381fec1d8e3cf1cd0c98c1246391c4e23ed71be87e3905f
jogeshgupta963/auth:75ab7595fa2f8afe6894ed4cb2ca9cdd33c710567cfb1dca55ac331b6fd5bf3d
jogeshgupta963/auth:b126efa-dirty
jogeshgupta963/auth:289e632f48da5a72255ae8228f66c83dc4c54d7938f2f3ce7962149771b5c40e
jogeshgupta963/auth:67c854420b61c54aca829ccd5559e0f317735cc270703f3c16f3ac3d626dc2f5
jogeshgupta963/auth:b126efa
mongo:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1209 02:34:35.229583    2791 cache_images.go:84] Images are preloaded, skipping loading
I1209 02:34:35.229610    2791 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1209 02:34:35.330979    2791 cni.go:95] Creating CNI manager for ""
I1209 02:34:35.330989    2791 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1209 02:34:35.330997    2791 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1209 02:34:35.331014    2791 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1209 02:34:35.331096    2791 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1209 02:34:35.331146    2791 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1209 02:34:35.331176    2791 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I1209 02:34:35.336866    2791 binaries.go:44] Found k8s binaries, skipping transfer
I1209 02:34:35.336899    2791 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1209 02:34:35.340437    2791 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1209 02:34:35.347302    2791 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1209 02:34:35.353803    2791 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1209 02:34:35.360970    2791 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1209 02:34:35.362531    2791 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1209 02:34:35.367387    2791 certs.go:54] Setting up /home/jogesh/.minikube/profiles/minikube for IP: 192.168.49.2
I1209 02:34:35.367465    2791 certs.go:182] skipping minikubeCA CA generation: /home/jogesh/.minikube/ca.key
I1209 02:34:35.367575    2791 certs.go:182] skipping proxyClientCA CA generation: /home/jogesh/.minikube/proxy-client-ca.key
I1209 02:34:35.367628    2791 certs.go:298] skipping minikube-user signed cert generation: /home/jogesh/.minikube/profiles/minikube/client.key
I1209 02:34:35.367732    2791 certs.go:298] skipping minikube signed cert generation: /home/jogesh/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1209 02:34:35.367828    2791 certs.go:298] skipping aggregator signed cert generation: /home/jogesh/.minikube/profiles/minikube/proxy-client.key
I1209 02:34:35.367888    2791 certs.go:388] found cert: /home/jogesh/.minikube/certs/home/jogesh/.minikube/certs/ca-key.pem (1675 bytes)
I1209 02:34:35.367907    2791 certs.go:388] found cert: /home/jogesh/.minikube/certs/home/jogesh/.minikube/certs/ca.pem (1078 bytes)
I1209 02:34:35.367923    2791 certs.go:388] found cert: /home/jogesh/.minikube/certs/home/jogesh/.minikube/certs/cert.pem (1123 bytes)
I1209 02:34:35.367937    2791 certs.go:388] found cert: /home/jogesh/.minikube/certs/home/jogesh/.minikube/certs/key.pem (1675 bytes)
I1209 02:34:35.368507    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1209 02:34:35.377631    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1209 02:34:35.386647    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1209 02:34:35.396087    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1209 02:34:35.405530    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1209 02:34:35.414271    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1209 02:34:35.423379    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1209 02:34:35.432720    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1209 02:34:35.441779    2791 ssh_runner.go:362] scp /home/jogesh/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1209 02:34:35.450984    2791 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1209 02:34:35.457662    2791 ssh_runner.go:195] Run: openssl version
I1209 02:34:35.461108    2791 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1209 02:34:35.465322    2791 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1209 02:34:35.466895    2791 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec  8 04:24 /usr/share/ca-certificates/minikubeCA.pem
I1209 02:34:35.466912    2791 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1209 02:34:35.469327    2791 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1209 02:34:35.472887    2791 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/jogesh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1209 02:34:35.472967    2791 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1209 02:34:35.486375    2791 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1209 02:34:35.490747    2791 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I1209 02:34:35.490757    2791 kubeadm.go:627] restartCluster start
I1209 02:34:35.490793    2791 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1209 02:34:35.494815    2791 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1209 02:34:35.495259    2791 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I1209 02:34:35.498155    2791 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1209 02:34:35.503380    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:35.503414    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:35.509176    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:35.709828    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:35.709861    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:35.715634    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:35.909834    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:35.909878    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:35.915160    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:36.109847    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:36.109881    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:36.115560    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:36.309866    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:36.309902    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:36.315839    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:36.509524    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:36.509562    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:36.515595    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:36.710280    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:36.710501    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:36.715804    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:36.909550    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:36.909593    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:36.915481    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:37.109568    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:37.109612    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:37.115053    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:37.310116    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:37.310149    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:37.315322    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:37.509951    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:37.509983    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:37.516067    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:37.709739    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:37.709783    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:37.715166    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:37.909909    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:37.909956    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:37.915679    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.109372    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:38.109450    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:38.115107    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.309914    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:38.310020    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:38.316468    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.509432    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:38.509477    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:38.515329    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.515338    2791 api_server.go:165] Checking apiserver status ...
I1209 02:34:38.515360    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1209 02:34:38.519974    2791 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.519992    2791 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I1209 02:34:38.519997    2791 kubeadm.go:1114] stopping kube-system containers ...
I1209 02:34:38.520025    2791 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1209 02:34:38.535462    2791 docker.go:444] Stopping containers: [9921c5a92e82 ed615297becf 6a6fb621d87a 606d3a130507 3257e33e16e5 4a71ae160764 291dc30f558f f7d4ed7d92eb 0144245707ca 331d200662ea 02a0787f5e98 ef97cc4087b6 c70c12c11e29 08e7982a3851 be8ca9fb7c73 5a15d273218e cc7974ac8bb1 4dc90d18a8e3 973909a28d58 483575b7a0eb 729ed4e622e7 18afed36cd61 f28123aeb22b d6c75493f854 d8a7e1423079 cb31e0971edc cc9da313391a 00742025d6b9]
I1209 02:34:38.535508    2791 ssh_runner.go:195] Run: docker stop 9921c5a92e82 ed615297becf 6a6fb621d87a 606d3a130507 3257e33e16e5 4a71ae160764 291dc30f558f f7d4ed7d92eb 0144245707ca 331d200662ea 02a0787f5e98 ef97cc4087b6 c70c12c11e29 08e7982a3851 be8ca9fb7c73 5a15d273218e cc7974ac8bb1 4dc90d18a8e3 973909a28d58 483575b7a0eb 729ed4e622e7 18afed36cd61 f28123aeb22b d6c75493f854 d8a7e1423079 cb31e0971edc cc9da313391a 00742025d6b9
I1209 02:34:38.549490    2791 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1209 02:34:38.555585    2791 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1209 02:34:38.559384    2791 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Dec  8 04:24 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Dec  8 20:02 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Dec  8 04:24 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Dec  8 20:02 /etc/kubernetes/scheduler.conf

I1209 02:34:38.559408    2791 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1209 02:34:38.563893    2791 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1209 02:34:38.568239    2791 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1209 02:34:38.572266    2791 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.572291    2791 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1209 02:34:38.576047    2791 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1209 02:34:38.579751    2791 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1209 02:34:38.579776    2791 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1209 02:34:38.583608    2791 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1209 02:34:38.587531    2791 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1209 02:34:38.587543    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:38.651309    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:39.195235    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:39.370615    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:39.397478    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:39.426343    2791 api_server.go:51] waiting for apiserver process to appear ...
I1209 02:34:39.426385    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 02:34:39.933325    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 02:34:40.433752    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 02:34:40.933571    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 02:34:40.939162    2791 api_server.go:71] duration metric: took 1.512821124s to wait for apiserver process to appear ...
I1209 02:34:40.939176    2791 api_server.go:87] waiting for apiserver healthz status ...
I1209 02:34:40.939191    2791 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1209 02:34:42.958632    2791 api_server.go:278] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1209 02:34:42.958649    2791 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1209 02:34:43.459351    2791 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1209 02:34:43.463632    2791 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1209 02:34:43.463656    2791 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1209 02:34:43.959678    2791 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1209 02:34:43.964929    2791 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1209 02:34:43.964941    2791 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1209 02:34:44.458728    2791 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1209 02:34:44.462139    2791 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1209 02:34:44.474748    2791 api_server.go:140] control plane version: v1.25.3
I1209 02:34:44.474758    2791 api_server.go:130] duration metric: took 3.535574009s to wait for apiserver health ...
I1209 02:34:44.474767    2791 cni.go:95] Creating CNI manager for ""
I1209 02:34:44.474773    2791 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1209 02:34:44.474782    2791 system_pods.go:43] waiting for kube-system pods to appear ...
I1209 02:34:44.480882    2791 system_pods.go:59] 7 kube-system pods found
I1209 02:34:44.480893    2791 system_pods.go:61] "coredns-565d847f94-66t6s" [b40777cd-0e9d-464b-a67d-36b85582091d] Running
I1209 02:34:44.480900    2791 system_pods.go:61] "etcd-minikube" [9788620c-fa05-4007-8909-fcccbc0d37fe] Running
I1209 02:34:44.480905    2791 system_pods.go:61] "kube-apiserver-minikube" [bb7fb973-1a4a-4a8f-800a-23c0eba291b6] Running
I1209 02:34:44.480910    2791 system_pods.go:61] "kube-controller-manager-minikube" [1eb4b361-3f6d-44c5-ab13-7139d4afd6d7] Running
I1209 02:34:44.480916    2791 system_pods.go:61] "kube-proxy-9htlb" [13b18e86-a52a-4fdc-95bb-72ee01a5252f] Running
I1209 02:34:44.480921    2791 system_pods.go:61] "kube-scheduler-minikube" [e4c61d8f-76c1-4eda-ac0e-6c514be48651] Running
I1209 02:34:44.480925    2791 system_pods.go:61] "storage-provisioner" [3bd9e57b-4dc6-4be5-ae09-4b62d002358c] Running
I1209 02:34:44.480931    2791 system_pods.go:74] duration metric: took 6.142541ms to wait for pod list to return data ...
I1209 02:34:44.480937    2791 node_conditions.go:102] verifying NodePressure condition ...
I1209 02:34:44.482744    2791 node_conditions.go:122] node storage ephemeral capacity is 101213192Ki
I1209 02:34:44.482756    2791 node_conditions.go:123] node cpu capacity is 16
I1209 02:34:44.482766    2791 node_conditions.go:105] duration metric: took 1.822369ms to run NodePressure ...
I1209 02:34:44.482782    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1209 02:34:44.559822    2791 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1209 02:34:44.563431    2791 ops.go:34] apiserver oom_adj: -16
I1209 02:34:44.563441    2791 kubeadm.go:631] restartCluster took 9.072675576s
I1209 02:34:44.563449    2791 kubeadm.go:398] StartCluster complete in 9.090567027s
I1209 02:34:44.563461    2791 settings.go:142] acquiring lock: {Name:mk845a156be6b387a2fc68720c146cc035e00b52 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 02:34:44.563529    2791 settings.go:150] Updating kubeconfig:  /home/jogesh/.kube/config
I1209 02:34:44.563925    2791 lock.go:35] WriteFile acquiring /home/jogesh/.kube/config: {Name:mka02726ef5da2a0faa209e9bb76d6dc0c5e96eb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1209 02:34:44.565840    2791 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1209 02:34:44.565872    2791 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1209 02:34:44.566481    2791 out.go:177] 🔎  Verifying Kubernetes components...
I1209 02:34:44.565956    2791 addons.go:486] enableAddons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1209 02:34:44.567346    2791 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1209 02:34:44.566521    2791 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1209 02:34:44.567397    2791 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W1209 02:34:44.567402    2791 addons.go:236] addon storage-provisioner should already be in state true
I1209 02:34:44.567425    2791 host.go:66] Checking if "minikube" exists ...
I1209 02:34:44.566027    2791 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1209 02:34:44.566020    2791 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1209 02:34:44.566537    2791 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1209 02:34:44.567534    2791 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1209 02:34:44.567624    2791 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 02:34:44.567689    2791 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 02:34:44.573746    2791 api_server.go:51] waiting for apiserver process to appear ...
I1209 02:34:44.573782    2791 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1209 02:34:44.583471    2791 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1209 02:34:44.584027    2791 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1209 02:34:44.584036    2791 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1209 02:34:44.584073    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:44.587016    2791 addons.go:227] Setting addon default-storageclass=true in "minikube"
W1209 02:34:44.587026    2791 addons.go:236] addon default-storageclass should already be in state true
I1209 02:34:44.587042    2791 host.go:66] Checking if "minikube" exists ...
I1209 02:34:44.587266    2791 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1209 02:34:44.599322    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:44.601854    2791 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I1209 02:34:44.601863    2791 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1209 02:34:44.601902    2791 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1209 02:34:44.614214    2791 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/jogesh/.minikube/machines/minikube/id_rsa Username:docker}
I1209 02:34:44.652750    2791 api_server.go:71] duration metric: took 86.859093ms to wait for apiserver process to appear ...
I1209 02:34:44.652756    2791 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1209 02:34:44.652764    2791 api_server.go:87] waiting for apiserver healthz status ...
I1209 02:34:44.652777    2791 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1209 02:34:44.655588    2791 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1209 02:34:44.656084    2791 api_server.go:140] control plane version: v1.25.3
I1209 02:34:44.656091    2791 api_server.go:130] duration metric: took 3.320395ms to wait for apiserver health ...
I1209 02:34:44.656097    2791 system_pods.go:43] waiting for kube-system pods to appear ...
I1209 02:34:44.658990    2791 system_pods.go:59] 7 kube-system pods found
I1209 02:34:44.658999    2791 system_pods.go:61] "coredns-565d847f94-66t6s" [b40777cd-0e9d-464b-a67d-36b85582091d] Running
I1209 02:34:44.659004    2791 system_pods.go:61] "etcd-minikube" [9788620c-fa05-4007-8909-fcccbc0d37fe] Running
I1209 02:34:44.659009    2791 system_pods.go:61] "kube-apiserver-minikube" [bb7fb973-1a4a-4a8f-800a-23c0eba291b6] Running
I1209 02:34:44.659014    2791 system_pods.go:61] "kube-controller-manager-minikube" [1eb4b361-3f6d-44c5-ab13-7139d4afd6d7] Running
I1209 02:34:44.659018    2791 system_pods.go:61] "kube-proxy-9htlb" [13b18e86-a52a-4fdc-95bb-72ee01a5252f] Running
I1209 02:34:44.659021    2791 system_pods.go:61] "kube-scheduler-minikube" [e4c61d8f-76c1-4eda-ac0e-6c514be48651] Running
I1209 02:34:44.659028    2791 system_pods.go:61] "storage-provisioner" [3bd9e57b-4dc6-4be5-ae09-4b62d002358c] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1209 02:34:44.659034    2791 system_pods.go:74] duration metric: took 2.931378ms to wait for pod list to return data ...
I1209 02:34:44.659042    2791 kubeadm.go:573] duration metric: took 93.153329ms to wait for : map[apiserver:true system_pods:true] ...
I1209 02:34:44.659051    2791 node_conditions.go:102] verifying NodePressure condition ...
I1209 02:34:44.660357    2791 node_conditions.go:122] node storage ephemeral capacity is 101213192Ki
I1209 02:34:44.660367    2791 node_conditions.go:123] node cpu capacity is 16
I1209 02:34:44.660376    2791 node_conditions.go:105] duration metric: took 1.318533ms to run NodePressure ...
I1209 02:34:44.660384    2791 start.go:217] waiting for startup goroutines ...
I1209 02:34:44.679483    2791 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1209 02:34:44.692266    2791 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1209 02:34:45.069623    2791 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1209 02:34:45.070091    2791 addons.go:488] enableAddons completed in 504.140451ms
I1209 02:34:45.070238    2791 ssh_runner.go:195] Run: rm -f paused
I1209 02:34:45.134929    2791 start.go:506] kubectl: 1.25.4, cluster: 1.25.3 (minor skew: 0)
I1209 02:34:45.135639    2791 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Thu 2022-12-08 21:04:29 UTC, end at Thu 2022-12-08 21:23:45 UTC. --
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.779964486Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint a330fb4992a11ca92d33dde0b4712811033612adef482ae21c5867fcc2300144 4c8d2cf7ae173271dae37d00d6da3da9cc619abea408c059f1579a75f73153a0], retrying...."
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.817373225Z" level=info msg="Removing stale sandbox 335010db06a1b5c1138c63556d10482cff9f7ea63c8752e757b833fdaf83ac14 (4a71ae160764766952726a7ffcf9ab51996a38f122cf987f183f3c34c2dc52d6)"
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.820042698Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint a330fb4992a11ca92d33dde0b4712811033612adef482ae21c5867fcc2300144 8045a1074ce7431ea465b68bc6d02223a01378d0d8db97e33af5063f126d6340], retrying...."
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.859137964Z" level=info msg="Removing stale sandbox 62e6512a7ded46c7dd8578f40f852f65156b743bab3e83cc50aed8e6bccf0bf8 (ed615297becfb72e54869986762113aae5f8bc606c5cc41dc45e56671a642fb8)"
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.859996733Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 206bc40c4335a01f4b64c4eae7d40e362933fab46360e38f616c777ba73f9821 640779e3a14305ab4e24d0f216c706833ea41a7167cee3aa997dde28a58e1ee8], retrying...."
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.898774919Z" level=info msg="Removing stale sandbox 9c270b40d94d867a2fe1588a9dfc88f06ba4a43d5411dd4146bdf18872cac19e (ef97cc4087b6d56a6933cd2b5a3298b9eff264089939a1de7c69a9012d321416)"
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.899545478Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 206bc40c4335a01f4b64c4eae7d40e362933fab46360e38f616c777ba73f9821 4af558c25984ac4ba138e4425290bbbd574a9230d5172e0635541ffb213a5243], retrying...."
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.938063436Z" level=info msg="Removing stale sandbox b3cf4de5642db0b430c2269072b6067b77ea9d0a7537ca7fcd69a0f94d44e68b (02a0787f5e98da9d10f1dbd2bab22c2cfe6957e93fd5944ca8422d0bbd7004d0)"
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.938870592Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 206bc40c4335a01f4b64c4eae7d40e362933fab46360e38f616c777ba73f9821 5a769ffa0b66a2d33d4c36111af1348b42fca9607b5734cdb705f6a18a48b1d1], retrying...."
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.976964124Z" level=info msg="Removing stale sandbox f52acb4d158de5e2aed9c299a3cb3490594d24888b64ac7e91e1cd918f6a426e (3257e33e16e54582e1913c34ee07683afa5b0e670d30e4d414a65c3f18dfc91a)"
Dec 08 21:04:29 minikube dockerd[137]: time="2022-12-08T21:04:29.977849083Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 206bc40c4335a01f4b64c4eae7d40e362933fab46360e38f616c777ba73f9821 db51bedd5eda2e84dcb445301174dcacb7ecb0e0bf18bb01c43ae6aab2c34798], retrying...."
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.272529023Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.306607729Z" level=info msg="Loading containers: done."
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.323911746Z" level=warning msg="Not using native diff for overlay2, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled" storage-driver=overlay2
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.324106463Z" level=info msg="Docker daemon" commit=03df974 graphdriver(s)=overlay2 version=20.10.20
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.324465517Z" level=info msg="Daemon has completed initialization"
Dec 08 21:04:30 minikube systemd[1]: Started Docker Application Container Engine.
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.406524139Z" level=info msg="API listen on [::]:2376"
Dec 08 21:04:30 minikube dockerd[137]: time="2022-12-08T21:04:30.408960273Z" level=info msg="API listen on /var/run/docker.sock"
Dec 08 21:04:33 minikube systemd[1]: Stopping Docker Application Container Engine...
Dec 08 21:04:33 minikube dockerd[137]: time="2022-12-08T21:04:33.879617854Z" level=info msg="Processing signal 'terminated'"
Dec 08 21:04:33 minikube dockerd[137]: time="2022-12-08T21:04:33.880795448Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Dec 08 21:04:33 minikube dockerd[137]: time="2022-12-08T21:04:33.881338534Z" level=info msg="Daemon shutdown complete"
Dec 08 21:04:33 minikube dockerd[137]: time="2022-12-08T21:04:33.881355715Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Dec 08 21:04:33 minikube systemd[1]: docker.service: Succeeded.
Dec 08 21:04:33 minikube systemd[1]: Stopped Docker Application Container Engine.
Dec 08 21:04:33 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.940088744Z" level=info msg="Starting up"
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941098579Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941113874Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941133220Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941143836Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941825348Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941837360Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941850211Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.941859710Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.952007090Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Dec 08 21:04:33 minikube dockerd[677]: time="2022-12-08T21:04:33.960969403Z" level=info msg="Loading containers: start."
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.637636547Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.729010110Z" level=info msg="Loading containers: done."
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.737655204Z" level=warning msg="Not using native diff for overlay2, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled" storage-driver=overlay2
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.737789090Z" level=info msg="Docker daemon" commit=03df974 graphdriver(s)=overlay2 version=20.10.20
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.737833928Z" level=info msg="Daemon has completed initialization"
Dec 08 21:04:34 minikube systemd[1]: Started Docker Application Container Engine.
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.789432530Z" level=info msg="API listen on [::]:2376"
Dec 08 21:04:34 minikube dockerd[677]: time="2022-12-08T21:04:34.791526302Z" level=info msg="API listen on /var/run/docker.sock"
Dec 08 21:04:55 minikube dockerd[677]: time="2022-12-08T21:04:55.070758002Z" level=info msg="ignoring event" container=5030bf98d40b118ccb9e82e6b9ccaa4f79e661d553be324222a9d6404b61e4a1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:15:21 minikube dockerd[677]: time="2022-12-08T21:15:21.561267572Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 08 21:15:31 minikube dockerd[677]: time="2022-12-08T21:15:31.169341847Z" level=info msg="ignoring event" container=74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:15:31 minikube dockerd[677]: time="2022-12-08T21:15:31.250901675Z" level=info msg="ignoring event" container=23f532346e3e112a10a7809477d1bb1fa4d9f1fcef539f36042fea0f8495a8c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.510679829Z" level=info msg="ignoring event" container=0effb39ac182d28946183bca6624002e06592b1c8be2abf1d28f6a603d3f80b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.529705707Z" level=info msg="ignoring event" container=7de0881b155aee79a4c9d7ebaf3c0da1d085ba6051dc655dac2ab671be94a894 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.573104482Z" level=info msg="ignoring event" container=6a3ebf39f07a71c603d2d954c86857eec910608456207fb4d840aca358ad5434 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.574836837Z" level=info msg="ignoring event" container=48ca69270ba00b3f13ac4bbde853659f25f1aa311632423eb08de539417b9fb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.579245250Z" level=info msg="ignoring event" container=f406e682aa113fc10d009ed4a10dabc0e2317b7e09bdd1ef3e5ea194921de7cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.642246726Z" level=info msg="ignoring event" container=490c4912e44ac97ea930a1422332e2a7bafd9d8c3d5aefcbf0f4e81c7c0bccd8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.678099864Z" level=info msg="ignoring event" container=b3d185da4df04dbe5b5eed34499a15a77aa9728ed21d7ba27fe1f4eecb8e995b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:18:30 minikube dockerd[677]: time="2022-12-08T21:18:30.678141839Z" level=info msg="ignoring event" container=5b6829a9774d9ddbfb04c5f6f85208c6dcb3f72cbe24efb45e56208b56ec6f63 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:19:24 minikube dockerd[677]: time="2022-12-08T21:19:24.476458292Z" level=info msg="ignoring event" container=8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 08 21:19:24 minikube dockerd[677]: time="2022-12-08T21:19:24.540505871Z" level=info msg="ignoring event" container=3e458fbee17a64bf121fd89f3a0a00b9a27551507b5b45ef71952701c01e9469 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID
37ea681efb1ca       k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8   7 minutes ago       Running             controller                0                   606cfbbb666ed
307aba3820ea5       6e38f40d628db                                                                                                 18 minutes ago      Running             storage-provisioner       5                   a0f698cbb5f04
0bdae608cc898       5cefe1229065b                                                                                                 19 minutes ago      Running             mongo                     2                   dd607f56678af
8ace1dc9037d5       beaaf00edd38a                                                                                                 19 minutes ago      Running             kube-proxy                3                   412095fdf4e94
8c80393c3378a       5185b96f0becf                                                                                                 19 minutes ago      Running             coredns                   3                   7fd7584e4ae32
5030bf98d40b1       6e38f40d628db                                                                                                 19 minutes ago      Exited              storage-provisioner       4                   a0f698cbb5f04
473c6a193836d       6039992312758                                                                                                 19 minutes ago      Running             kube-controller-manager   3                   3f50e3e8f0eb0
50ba86c8f3741       0346dbd74bcb9                                                                                                 19 minutes ago      Running             kube-apiserver            3                   eacd4c9113828
2812f1ad7bcba       6d23ec0e8b87e                                                                                                 19 minutes ago      Running             kube-scheduler            3                   cab3c168dbbd6
ddbd67bcc7969       a8a176a5d5d69                                                                                                 19 minutes ago      Running             etcd                      3                   f1f688b97aa91
6813d0968fe34       5cefe1229065b                                                                                                 About an hour ago   Exited              mongo                     1                   283ed0bc2980d
6a6fb621d87ac       5185b96f0becf                                                                                                 About an hour ago   Exited              coredns                   2                   4a71ae1607647
606d3a1305074       beaaf00edd38a                                                                                                 About an hour ago   Exited              kube-proxy                2                   3257e33e16e54
291dc30f558f9       6d23ec0e8b87e                                                                                                 About an hour ago   Exited              kube-scheduler            2                   02a0787f5e98d
f7d4ed7d92eb5       0346dbd74bcb9                                                                                                 About an hour ago   Exited              kube-apiserver            2                   c70c12c11e297
0144245707cae       a8a176a5d5d69                                                                                                 About an hour ago   Exited              etcd                      2                   08e7982a38515
331d200662eac       6039992312758                                                                                                 About an hour ago   Exited              kube-controller-manager   2                   ef97cc4087b6d

* 
* ==> coredns [6a6fb621d87a] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [8c80393c3378] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_08T09_54_22_0700
                    minikube.k8s.io/version=v1.28.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 08 Dec 2022 04:24:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 08 Dec 2022 21:23:38 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 08 Dec 2022 21:21:55 +0000   Thu, 08 Dec 2022 04:24:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 08 Dec 2022 21:21:55 +0000   Thu, 08 Dec 2022 04:24:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 08 Dec 2022 21:21:55 +0000   Thu, 08 Dec 2022 04:24:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 08 Dec 2022 21:21:55 +0000   Thu, 08 Dec 2022 04:24:22 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  101213192Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15695888Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  101213192Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15695888Ki
  pods:               110
System Info:
  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9
  System UUID:                f4e74b35-af53-437e-abc9-39d58f32328c
  Boot ID:                    c6f144af-3a89-4bab-b024-9ccfaa6049f3
  Kernel Version:             6.0.10-arch2-1
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     ticekts-mongo-depl-5fd7d89469-wmk9c          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h47m
  ingress-nginx               ingress-nginx-controller-5959f988fd-m4gkz    100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         8m25s
  kube-system                 coredns-565d847f94-66t6s                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     16h
  kube-system                 etcd-minikube                                100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         16h
  kube-system                 kube-apiserver-minikube                      250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  kube-system                 kube-controller-manager-minikube             200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  kube-system                 kube-proxy-9htlb                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  kube-system                 kube-scheduler-minikube                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%!)(MISSING)   0 (0%!)(MISSING)
  memory             260Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 80m                kube-proxy       
  Normal  Starting                 19m                kube-proxy       
  Normal  NodeAllocatableEnforced  81m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  81m (x8 over 81m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    81m (x8 over 81m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     81m (x7 over 81m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 81m                kubelet          Starting kubelet.
  Normal  RegisteredNode           80m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 19m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     19m (x7 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  19m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           18m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Dec 8 21:01] acpi PNP0C14:01: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:00)
[  +0.000206] acpi PNP0C14:02: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:00)
[  +0.012959] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.465686] r8168: loading out-of-tree module taints kernel.
[  +0.016265] r8168  Copyright (C) 2022 Realtek NIC software team <nicfae@realtek.com> 
               This program comes with ABSOLUTELY NO WARRANTY; for details, please see <http://www.gnu.org/licenses/>. 
               This is free software, and you are welcome to redistribute it under certain conditions; see <http://www.gnu.org/licenses/>. 
[  +0.205029] nvidia: module license 'NVIDIA' taints kernel.
[  +0.000002] Disabling lock debugging due to kernel taint

[  +0.350844] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  525.60.11  Wed Nov 23 23:04:03 UTC 2022
[  +0.122032] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.
[  +0.059069] clocksource: timekeeping watchdog on CPU4: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.000002] clocksource:                       'hpet' wd_nsec: 504167987 wd_now: 2298234 wd_last: 1bb5be4 mask: ffffffff
[  +0.000002] clocksource:                       'tsc' cs_nsec: 503325732 cs_now: 6af130d60 cs_last: 64f179fe0 mask: ffffffffffffffff
[  +0.000001] clocksource:                       'tsc' is current clocksource.
[  +0.000015] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.000103] clocksource: Checking clocksource tsc synchronization from CPU 6 to CPUs 0,4-5,7,14.
[  +0.238314] ACPI Warning: \_SB.NPCF._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20220331/nsarguments-61)
[  +0.000052] ACPI Warning: \_SB.PCI0.GPP0.PEGP._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20220331/nsarguments-61)
[  +0.309706] Bluetooth: hci0: HCI Enhanced Setup Synchronous Connection command is advertised, but not supported.
[  +0.283473] Bluetooth: hci0: AOSP quality report is not supported
[  +0.827692] ATPX version 1, functions 0x00000201
[  +0.000031] ATPX Hybrid Graphics
[  +0.066824] amdgpu 0000:06:00.0: amdgpu: PSP runtime database doesn't exist
[  +0.000002] amdgpu 0000:06:00.0: amdgpu: PSP runtime database doesn't exist
[  +1.048703] amdgpu: SRAT table not found
[  +0.148651] kauditd_printk_skb: 36 callbacks suppressed
[  +7.648890] kauditd_printk_skb: 9 callbacks suppressed
[Dec 8 21:04] kauditd_printk_skb: 12 callbacks suppressed
[ +18.272037] kauditd_printk_skb: 176 callbacks suppressed
[  +5.079992] kauditd_printk_skb: 359 callbacks suppressed
[  +5.414081] kauditd_printk_skb: 638 callbacks suppressed
[  +5.291321] kauditd_printk_skb: 258 callbacks suppressed
[  +0.896598] Invalid ELF header magic: != ELF
[  +0.001419] Invalid ELF header magic: != ELF
[  +0.001050] Invalid ELF header magic: != ELF
[  +0.000926] Invalid ELF header magic: != ELF
[  +9.678221] kauditd_printk_skb: 331 callbacks suppressed
[  +5.749698] kauditd_printk_skb: 10 callbacks suppressed
[Dec 8 21:05] kauditd_printk_skb: 110 callbacks suppressed
[  +5.789370] kauditd_printk_skb: 30 callbacks suppressed
[ +17.693807] kauditd_printk_skb: 4 callbacks suppressed
[Dec 8 21:15] kauditd_printk_skb: 2 callbacks suppressed
[  +6.531115] kauditd_printk_skb: 85 callbacks suppressed
[  +5.008015] kauditd_printk_skb: 12 callbacks suppressed
[ +11.430027] kauditd_printk_skb: 11 callbacks suppressed
[Dec 8 21:16] kauditd_printk_skb: 6 callbacks suppressed
[  +8.900689] kauditd_printk_skb: 8 callbacks suppressed
[ +18.968709] kauditd_printk_skb: 2 callbacks suppressed
[Dec 8 21:17] kauditd_printk_skb: 38 callbacks suppressed
[  +8.599441] kauditd_printk_skb: 8 callbacks suppressed
[Dec 8 21:18] kauditd_printk_skb: 30 callbacks suppressed
[Dec 8 21:19] kauditd_printk_skb: 2 callbacks suppressed
[  +5.054097] kauditd_printk_skb: 69 callbacks suppressed
[  +5.354428] kauditd_printk_skb: 14 callbacks suppressed
[  +5.043508] kauditd_printk_skb: 12 callbacks suppressed
[ +11.394845] kauditd_printk_skb: 11 callbacks suppressed
[Dec 8 21:22] kauditd_printk_skb: 38 callbacks suppressed
[  +8.618107] kauditd_printk_skb: 8 callbacks suppressed

* 
* ==> etcd [0144245707ca] <==
* {"level":"info","ts":"2022-12-08T20:02:45.424Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-12-08T20:02:45.425Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-12-08T20:02:45.425Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":12407}
{"level":"info","ts":"2022-12-08T20:02:45.427Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":12858}
{"level":"info","ts":"2022-12-08T20:02:45.428Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-12-08T20:02:45.429Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2022-12-08T20:02:45.429Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-12-08T20:02:45.429Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.4","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-08T20:02:45.429Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-12-08T20:02:45.431Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-08T20:02:45.431Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-12-08T20:02:45.431Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-12-08T20:02:45.431Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T20:02:45.431Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2022-12-08T20:02:45.725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2022-12-08T20:02:45.726Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-12-08T20:02:45.726Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-08T20:02:45.726Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-08T20:02:45.726Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-12-08T20:02:45.726Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-12-08T20:02:45.727Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-12-08T20:02:45.727Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-12-08T20:12:45.744Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13406}
{"level":"info","ts":"2022-12-08T20:12:45.757Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13406,"took":"12.538997ms"}
{"level":"info","ts":"2022-12-08T20:17:45.759Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13977}
{"level":"info","ts":"2022-12-08T20:17:45.773Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":13977,"took":"13.448797ms"}
{"level":"info","ts":"2022-12-08T20:22:45.774Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14377}
{"level":"info","ts":"2022-12-08T20:22:45.786Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14377,"took":"11.394704ms"}
{"level":"info","ts":"2022-12-08T20:27:45.791Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14704}
{"level":"info","ts":"2022-12-08T20:27:45.803Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14704,"took":"11.647708ms"}
{"level":"info","ts":"2022-12-08T20:32:45.806Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15030}
{"level":"info","ts":"2022-12-08T20:32:45.818Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15030,"took":"11.518392ms"}
{"level":"info","ts":"2022-12-08T20:37:45.821Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15578}
{"level":"info","ts":"2022-12-08T20:37:45.836Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15578,"took":"13.892726ms"}
{"level":"info","ts":"2022-12-08T20:42:45.836Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15900}
{"level":"info","ts":"2022-12-08T20:42:45.849Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15900,"took":"12.249611ms"}
{"level":"info","ts":"2022-12-08T20:47:45.853Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16151}
{"level":"info","ts":"2022-12-08T20:47:45.865Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16151,"took":"11.419638ms"}
{"level":"info","ts":"2022-12-08T20:50:30.183Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-12-08T20:50:30.186Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2022-12-08T20:50:30.186Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2022-12-08T20:52:45.868Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16402}
{"level":"info","ts":"2022-12-08T20:52:45.881Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16402,"took":"12.503068ms"}
{"level":"info","ts":"2022-12-08T20:57:45.884Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16822}
{"level":"info","ts":"2022-12-08T20:57:45.897Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16822,"took":"12.449838ms"}
{"level":"info","ts":"2022-12-08T21:00:54.824Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-12-08T21:00:54.825Z","caller":"embed/etcd.go:368","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2022-12-08T21:00:54.834Z","caller":"v3rpc/watch.go:436","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
WARNING: 2022/12/08 21:00:54 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2022/12/08 21:00:54 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-12-08T21:00:54.859Z","caller":"etcdserver/server.go:1453","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-12-08T21:00:54.861Z","caller":"embed/etcd.go:563","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T21:00:54.862Z","caller":"embed/etcd.go:568","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T21:00:54.862Z","caller":"embed/etcd.go:370","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [ddbd67bcc796] <==
* {"level":"info","ts":"2022-12-08T21:04:40.775Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-12-08T21:04:40.776Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2022-12-08T21:04:40.776Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-12-08T21:04:40.776Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-08T21:04:40.777Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-12-08T21:04:40.777Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.4","git-sha":"08407ff76","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-12-08T21:04:40.789Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"11.245005ms"}
{"level":"info","ts":"2022-12-08T21:04:40.837Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.9 kB"}
{"level":"info","ts":"2022-12-08T21:04:40.837Z","caller":"etcdserver/server.go:521","msg":"recovered v3 backend from snapshot","backend-size-bytes":8302592,"backend-size":"8.3 MB","backend-size-in-use-bytes":3354624,"backend-size-in-use":"3.4 MB"}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":20941}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 20941, applied: 20002, lastindex: 20941, lastterm: 4]"}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-12-08T21:04:40.876Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-12-08T21:04:40.877Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-12-08T21:04:40.877Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":16822}
{"level":"info","ts":"2022-12-08T21:04:40.879Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":17440}
{"level":"info","ts":"2022-12-08T21:04:40.880Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-12-08T21:04:40.881Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2022-12-08T21:04:40.881Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-12-08T21:04:40.882Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.4","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-08T21:04:40.882Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-12-08T21:04:40.884Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-08T21:04:40.884Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T21:04:40.884Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-08T21:04:40.885Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-12-08T21:04:40.885Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2022-12-08T21:04:41.684Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2022-12-08T21:04:41.685Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-12-08T21:04:41.685Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-08T21:04:41.685Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-08T21:04:41.685Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-12-08T21:04:41.685Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-12-08T21:04:41.686Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-12-08T21:04:41.686Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-12-08T21:14:41.707Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17892}
{"level":"info","ts":"2022-12-08T21:14:41.719Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17892,"took":"12.049628ms"}
{"level":"info","ts":"2022-12-08T21:19:41.723Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18147}
{"level":"info","ts":"2022-12-08T21:19:41.737Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18147,"took":"12.860249ms"}

* 
* ==> kernel <==
*  21:23:45 up 22 min,  0 users,  load average: 1.04, 0.66, 0.53
Linux minikube 6.0.10-arch2-1 #1 SMP PREEMPT_DYNAMIC Sat, 26 Nov 2022 16:51:18 +0000 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [50ba86c8f374] <==
* W1208 21:04:42.153248       1 genericapiserver.go:656] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1208 21:04:42.938998       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1208 21:04:42.939015       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1208 21:04:42.939117       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1208 21:04:42.939176       1 secure_serving.go:210] Serving securely on [::]:8443
I1208 21:04:42.939228       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1208 21:04:42.939257       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1208 21:04:42.939461       1 controller.go:83] Starting OpenAPI AggregationController
I1208 21:04:42.939540       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1208 21:04:42.939551       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1208 21:04:42.948213       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1208 21:04:42.948224       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1208 21:04:42.948222       1 available_controller.go:491] Starting AvailableConditionController
I1208 21:04:42.948238       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1208 21:04:42.948261       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1208 21:04:42.948302       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1208 21:04:42.948511       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1208 21:04:42.948598       1 autoregister_controller.go:141] Starting autoregister controller
I1208 21:04:42.948606       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1208 21:04:42.948666       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1208 21:04:42.948672       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1208 21:04:42.949170       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1208 21:04:42.950046       1 establishing_controller.go:76] Starting EstablishingController
I1208 21:04:42.950087       1 controller.go:85] Starting OpenAPI V3 controller
I1208 21:04:42.950110       1 naming_controller.go:291] Starting NamingConditionController
I1208 21:04:42.950175       1 controller.go:85] Starting OpenAPI controller
I1208 21:04:42.950228       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1208 21:04:42.950294       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1208 21:04:42.950316       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1208 21:04:42.950334       1 crd_finalizer.go:266] Starting CRDFinalizer
E1208 21:04:42.962380       1 controller.go:159] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I1208 21:04:43.039913       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1208 21:04:43.048265       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1208 21:04:43.048301       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1208 21:04:43.048610       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1208 21:04:43.048892       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1208 21:04:43.048907       1 cache.go:39] Caches are synced for autoregister controller
I1208 21:04:43.060330       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1208 21:04:43.062366       1 shared_informer.go:262] Caches are synced for node_authorizer
I1208 21:04:43.798053       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1208 21:04:43.962648       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1208 21:04:44.523583       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1208 21:04:44.529669       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1208 21:04:44.544535       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1208 21:04:44.553247       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1208 21:04:44.556355       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1208 21:04:55.226081       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1208 21:04:55.272017       1 controller.go:616] quota admission added evaluator for: endpoints
I1208 21:05:04.560228       1 controller.go:616] quota admission added evaluator for: replicasets.apps
I1208 21:05:04.563813       1 alloc.go:327] "allocated clusterIPs" service="default/auth-srv" clusterIPs=map[IPv4:10.102.72.95]
I1208 21:05:04.584153       1 alloc.go:327] "allocated clusterIPs" service="default/auth-mongo-srv" clusterIPs=map[IPv4:10.105.240.46]
I1208 21:05:04.606811       1 controller.go:616] quota admission added evaluator for: ingresses.networking.k8s.io
I1208 21:05:04.617985       1 alloc.go:327] "allocated clusterIPs" service="default/tickets-srv" clusterIPs=map[IPv4:10.108.204.89]
I1208 21:05:04.628136       1 alloc.go:327] "allocated clusterIPs" service="default/tickets-mongo-srv" clusterIPs=map[IPv4:10.107.189.141]
I1208 21:18:30.414112       1 trace.go:205] Trace[80920481]: "Get" url:/api/v1/namespaces/default/pods/tickets-depl-5b78bbd9c5-7x78c/log,user-agent:kubectl/v1.25.4 (linux/amd64) kubernetes/872a965,audit-id:6fbdac9c-c4b4-484b-a09f-aa0a90ab2fd8,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (08-Dec-2022 21:05:11.793) (total time: 798620ms):
Trace[80920481]: ---"Writing http response done" 798618ms (21:18:30.414)
Trace[80920481]: [13m18.620441353s] [13m18.620441353s] END
I1208 21:18:30.414148       1 trace.go:205] Trace[434431913]: "Get" url:/api/v1/namespaces/default/pods/auth-depl-645b99b58d-86hzl/log,user-agent:kubectl/v1.25.4 (linux/amd64) kubernetes/872a965,audit-id:ae82254a-86f2-450c-b183-3cbae5711b1f,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (08-Dec-2022 21:05:11.800) (total time: 798613ms):
Trace[434431913]: ---"Writing http response done" 798611ms (21:18:30.414)
Trace[434431913]: [13m18.613602346s] [13m18.613602346s] END

* 
* ==> kube-apiserver [f7d4ed7d92eb] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.852838       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.852883       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.852929       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.852982       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.853032       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.855372       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1208 21:00:54.858893       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [331d200662ea] <==
* I1208 20:28:02.461342       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-9bb8cf895 to 1"
I1208 20:28:02.463543       1 event.go:294] "Event occurred" object="default/tickets-depl-9bb8cf895" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-9bb8cf895-v6rwv"
I1208 20:28:03.462857       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-79c84f794d to 0 from 1"
I1208 20:28:03.465340       1 event.go:294] "Event occurred" object="default/tickets-depl-79c84f794d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-79c84f794d-s8zdz"
I1208 20:28:07.147567       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-579d4dd7cb to 1"
I1208 20:28:07.149563       1 event.go:294] "Event occurred" object="default/tickets-depl-579d4dd7cb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-579d4dd7cb-vhb9x"
I1208 20:28:08.512548       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-9bb8cf895 to 0 from 1"
I1208 20:28:08.515726       1 event.go:294] "Event occurred" object="default/tickets-depl-9bb8cf895" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-9bb8cf895-v6rwv"
I1208 20:28:16.197535       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-7fb89c8b9c to 1"
I1208 20:28:16.200579       1 event.go:294] "Event occurred" object="default/tickets-depl-7fb89c8b9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-7fb89c8b9c-2n894"
I1208 20:28:17.596914       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-579d4dd7cb to 0 from 1"
I1208 20:28:17.600419       1 event.go:294] "Event occurred" object="default/tickets-depl-579d4dd7cb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-579d4dd7cb-vhb9x"
I1208 20:29:24.216473       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-fd597b6d9 to 1"
I1208 20:29:24.219737       1 event.go:294] "Event occurred" object="default/tickets-depl-fd597b6d9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-fd597b6d9-dkrq6"
I1208 20:29:25.154131       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-7fb89c8b9c to 0 from 1"
I1208 20:29:25.157304       1 event.go:294] "Event occurred" object="default/tickets-depl-7fb89c8b9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-7fb89c8b9c-2n894"
I1208 20:29:27.166377       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-847c6cfd96 to 1"
I1208 20:29:27.168846       1 event.go:294] "Event occurred" object="default/tickets-depl-847c6cfd96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-847c6cfd96-tf5r8"
I1208 20:29:28.193768       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-fd597b6d9 to 0 from 1"
I1208 20:29:28.196520       1 event.go:294] "Event occurred" object="default/tickets-depl-fd597b6d9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-fd597b6d9-dkrq6"
I1208 20:29:33.176749       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-5d94d777bb to 1"
I1208 20:29:33.178628       1 event.go:294] "Event occurred" object="default/tickets-depl-5d94d777bb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-5d94d777bb-ps7ng"
I1208 20:29:34.257863       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-847c6cfd96 to 0 from 1"
I1208 20:29:34.261687       1 event.go:294] "Event occurred" object="default/tickets-depl-847c6cfd96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-847c6cfd96-tf5r8"
I1208 20:29:49.185520       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-c956d7d4 to 1"
I1208 20:29:49.187935       1 event.go:294] "Event occurred" object="default/tickets-depl-c956d7d4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-c956d7d4-q45mm"
I1208 20:29:50.390803       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-5d94d777bb to 0 from 1"
I1208 20:29:50.394552       1 event.go:294] "Event occurred" object="default/tickets-depl-5d94d777bb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-5d94d777bb-ps7ng"
I1208 20:33:29.174065       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-59b5f64c5f to 1"
I1208 20:33:29.177405       1 event.go:294] "Event occurred" object="default/tickets-depl-59b5f64c5f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-59b5f64c5f-7xlzf"
I1208 20:33:30.975998       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-c956d7d4 to 0 from 1"
I1208 20:33:30.979667       1 event.go:294] "Event occurred" object="default/tickets-depl-c956d7d4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-c956d7d4-q45mm"
I1208 20:37:04.164679       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-558db48d4 to 1"
I1208 20:37:04.168308       1 event.go:294] "Event occurred" object="default/tickets-depl-558db48d4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-558db48d4-bzt7q"
I1208 20:37:05.845719       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-59b5f64c5f to 0 from 1"
I1208 20:37:05.851212       1 event.go:294] "Event occurred" object="default/tickets-depl-59b5f64c5f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-59b5f64c5f-7xlzf"
I1208 20:50:30.159183       1 event.go:294] "Event occurred" object="default/auth-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-depl-65b494c594 to 1"
I1208 20:50:30.163441       1 event.go:294] "Event occurred" object="default/auth-depl-65b494c594" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-depl-65b494c594-kt757"
I1208 20:50:30.169266       1 event.go:294] "Event occurred" object="default/auth-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-mongo-depl-7dbfd4cb79 to 1"
I1208 20:50:30.171147       1 event.go:294] "Event occurred" object="default/auth-mongo-depl-7dbfd4cb79" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-mongo-depl-7dbfd4cb79-zxxrd"
I1208 20:50:30.180674       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-6c499cbd94 to 1"
I1208 20:50:30.182439       1 event.go:294] "Event occurred" object="default/tickets-depl-6c499cbd94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-6c499cbd94-kkbkl"
I1208 20:50:30.192299       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-mongo-depl-8557bb69c6 to 1"
I1208 20:50:30.194345       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl-8557bb69c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-mongo-depl-8557bb69c6-hwvmr"
I1208 20:59:34.150564       1 event.go:294] "Event occurred" object="default/auth-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-depl-796bc5dbfb to 1"
I1208 20:59:34.155946       1 event.go:294] "Event occurred" object="default/auth-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-mongo-depl-cdffddf74 to 1"
I1208 20:59:34.158442       1 event.go:294] "Event occurred" object="default/auth-depl-796bc5dbfb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-depl-796bc5dbfb-4kdgf"
I1208 20:59:34.159726       1 event.go:294] "Event occurred" object="default/auth-mongo-depl-cdffddf74" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-mongo-depl-cdffddf74-bdffn"
I1208 20:59:34.163289       1 event.go:294] "Event occurred" object="default/auth-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set auth-mongo-depl-7dbfd4cb79 to 0 from 1"
I1208 20:59:34.165708       1 event.go:294] "Event occurred" object="default/auth-mongo-depl-7dbfd4cb79" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: auth-mongo-depl-7dbfd4cb79-zxxrd"
I1208 20:59:34.170704       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-54779fc4c9 to 1"
I1208 20:59:34.173172       1 event.go:294] "Event occurred" object="default/tickets-depl-54779fc4c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-54779fc4c9-wsbjm"
I1208 20:59:34.183004       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-mongo-depl-7f699bd8b9 to 1"
I1208 20:59:34.186268       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl-7f699bd8b9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-mongo-depl-7f699bd8b9-6lsmz"
I1208 20:59:34.189847       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-mongo-depl-8557bb69c6 to 0 from 1"
I1208 20:59:34.195282       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl-8557bb69c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-mongo-depl-8557bb69c6-hwvmr"
I1208 20:59:38.175794       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set tickets-depl-6c499cbd94 to 0 from 1"
I1208 20:59:38.178492       1 event.go:294] "Event occurred" object="default/tickets-depl-6c499cbd94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: tickets-depl-6c499cbd94-kkbkl"
I1208 20:59:38.574284       1 event.go:294] "Event occurred" object="default/auth-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set auth-depl-65b494c594 to 0 from 1"
I1208 20:59:38.580661       1 event.go:294] "Event occurred" object="default/auth-depl-65b494c594" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: auth-depl-65b494c594-kt757"

* 
* ==> kube-controller-manager [473c6a193836] <==
* I1208 21:04:55.119746       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1208 21:04:55.119747       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I1208 21:04:55.119826       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1208 21:04:55.122056       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I1208 21:04:55.168377       1 shared_informer.go:262] Caches are synced for GC
I1208 21:04:55.170606       1 shared_informer.go:262] Caches are synced for ReplicationController
I1208 21:04:55.171982       1 shared_informer.go:262] Caches are synced for service account
I1208 21:04:55.174196       1 shared_informer.go:262] Caches are synced for node
I1208 21:04:55.174260       1 range_allocator.go:166] Starting range CIDR allocator
I1208 21:04:55.174302       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I1208 21:04:55.174318       1 shared_informer.go:262] Caches are synced for cidrallocator
I1208 21:04:55.175302       1 shared_informer.go:262] Caches are synced for job
I1208 21:04:55.180846       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I1208 21:04:55.183113       1 shared_informer.go:262] Caches are synced for TTL after finished
I1208 21:04:55.185257       1 shared_informer.go:262] Caches are synced for namespace
I1208 21:04:55.186384       1 shared_informer.go:262] Caches are synced for daemon sets
I1208 21:04:55.187604       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1208 21:04:55.188856       1 shared_informer.go:262] Caches are synced for TTL
I1208 21:04:55.191494       1 shared_informer.go:262] Caches are synced for deployment
I1208 21:04:55.192843       1 shared_informer.go:262] Caches are synced for taint
I1208 21:04:55.192875       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I1208 21:04:55.192879       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
I1208 21:04:55.192921       1 taint_manager.go:209] "Sending events to api server"
W1208 21:04:55.192933       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1208 21:04:55.192933       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1208 21:04:55.192965       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I1208 21:04:55.194353       1 shared_informer.go:262] Caches are synced for crt configmap
I1208 21:04:55.196270       1 shared_informer.go:262] Caches are synced for PV protection
I1208 21:04:55.202916       1 shared_informer.go:262] Caches are synced for HPA
I1208 21:04:55.218096       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1208 21:04:55.266841       1 shared_informer.go:262] Caches are synced for endpoint
I1208 21:04:55.290358       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1208 21:04:55.298964       1 shared_informer.go:262] Caches are synced for resource quota
I1208 21:04:55.304598       1 shared_informer.go:262] Caches are synced for disruption
I1208 21:04:55.315059       1 shared_informer.go:262] Caches are synced for attach detach
I1208 21:04:55.375629       1 shared_informer.go:262] Caches are synced for persistent volume
I1208 21:04:55.377895       1 shared_informer.go:262] Caches are synced for PVC protection
I1208 21:04:55.377956       1 shared_informer.go:262] Caches are synced for expand
I1208 21:04:55.392417       1 shared_informer.go:262] Caches are synced for stateful set
I1208 21:04:55.394778       1 shared_informer.go:262] Caches are synced for ephemeral
I1208 21:04:55.395944       1 shared_informer.go:262] Caches are synced for resource quota
I1208 21:04:55.709448       1 shared_informer.go:262] Caches are synced for garbage collector
I1208 21:04:55.774468       1 shared_informer.go:262] Caches are synced for garbage collector
I1208 21:04:55.774490       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1208 21:05:04.561342       1 event.go:294] "Event occurred" object="default/auth-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-depl-645b99b58d to 1"
I1208 21:05:04.564280       1 event.go:294] "Event occurred" object="default/auth-depl-645b99b58d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-depl-645b99b58d-86hzl"
I1208 21:05:04.574588       1 event.go:294] "Event occurred" object="default/auth-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set auth-mongo-depl-8565589478 to 1"
I1208 21:05:04.576450       1 event.go:294] "Event occurred" object="default/auth-mongo-depl-8565589478" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: auth-mongo-depl-8565589478-2vbtk"
I1208 21:05:04.615302       1 event.go:294] "Event occurred" object="default/tickets-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-depl-5b78bbd9c5 to 1"
I1208 21:05:04.617269       1 event.go:294] "Event occurred" object="default/tickets-depl-5b78bbd9c5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-depl-5b78bbd9c5-7x78c"
I1208 21:05:04.625092       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set tickets-mongo-depl-67d786fdf7 to 1"
I1208 21:05:04.627528       1 event.go:294] "Event occurred" object="default/tickets-mongo-depl-67d786fdf7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: tickets-mongo-depl-67d786fdf7-vcxvt"
I1208 21:15:20.010376       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5959f988fd to 1"
I1208 21:15:20.016103       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5959f988fd-m4gkz"
I1208 21:15:20.020383       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-8574b6d7c9 to 0 from 1"
I1208 21:15:20.024119       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-8574b6d7c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-8574b6d7c9-xlm5h"
I1208 21:19:08.822582       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-8574b6d7c9 to 1 from 0"
I1208 21:19:08.826889       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-8574b6d7c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-8574b6d7c9-fqnqc"
I1208 21:19:13.306447       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-8574b6d7c9 to 0 from 1"
I1208 21:19:13.308363       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-8574b6d7c9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-8574b6d7c9-fqnqc"

* 
* ==> kube-proxy [606d3a130507] <==
* I1208 20:02:51.265439       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1208 20:02:51.266749       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1208 20:02:51.267834       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1208 20:02:51.270053       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1208 20:02:51.281045       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1208 20:02:51.281072       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1208 20:02:51.281093       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1208 20:02:51.294514       1 server_others.go:206] "Using iptables Proxier"
I1208 20:02:51.294547       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1208 20:02:51.294557       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1208 20:02:51.294570       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1208 20:02:51.294734       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1208 20:02:51.294886       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1208 20:02:51.295076       1 server.go:661] "Version info" version="v1.25.3"
I1208 20:02:51.295086       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 20:02:51.297136       1 config.go:226] "Starting endpoint slice config controller"
I1208 20:02:51.297159       1 config.go:317] "Starting service config controller"
I1208 20:02:51.297218       1 config.go:444] "Starting node config controller"
I1208 20:02:51.297324       1 shared_informer.go:255] Waiting for caches to sync for service config
I1208 20:02:51.297427       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1208 20:02:51.297442       1 shared_informer.go:255] Waiting for caches to sync for node config
I1208 20:02:51.397891       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1208 20:02:51.397895       1 shared_informer.go:262] Caches are synced for node config
I1208 20:02:51.397923       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-proxy [8ace1dc9037d] <==
* I1208 21:04:45.383181       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1208 21:04:45.384298       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1208 21:04:45.385283       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1208 21:04:45.386192       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1208 21:04:45.394139       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1208 21:04:45.394155       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1208 21:04:45.394169       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1208 21:04:45.405178       1 server_others.go:206] "Using iptables Proxier"
I1208 21:04:45.405197       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1208 21:04:45.405205       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1208 21:04:45.405217       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1208 21:04:45.405347       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1208 21:04:45.405425       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1208 21:04:45.405584       1 server.go:661] "Version info" version="v1.25.3"
I1208 21:04:45.405592       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 21:04:45.406372       1 config.go:226] "Starting endpoint slice config controller"
I1208 21:04:45.406392       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1208 21:04:45.406411       1 config.go:444] "Starting node config controller"
I1208 21:04:45.406419       1 config.go:317] "Starting service config controller"
I1208 21:04:45.406428       1 shared_informer.go:255] Waiting for caches to sync for node config
I1208 21:04:45.406432       1 shared_informer.go:255] Waiting for caches to sync for service config
I1208 21:04:45.507218       1 shared_informer.go:262] Caches are synced for node config
I1208 21:04:45.507223       1 shared_informer.go:262] Caches are synced for service config
I1208 21:04:45.507240       1 shared_informer.go:262] Caches are synced for endpoint slice config
E1208 21:15:20.059229       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:30615: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E1208 21:19:13.329240       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:31754: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"

* 
* ==> kube-scheduler [2812f1ad7bcb] <==
* I1208 21:04:41.046089       1 serving.go:348] Generated self-signed cert in-memory
W1208 21:04:42.957363       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1208 21:04:42.957389       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1208 21:04:42.957403       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1208 21:04:42.957413       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1208 21:04:43.007164       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I1208 21:04:43.007179       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 21:04:43.007976       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1208 21:04:43.008016       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1208 21:04:43.008053       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1208 21:04:43.008069       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1208 21:04:43.108558       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [291dc30f558f] <==
* I1208 20:02:45.630706       1 serving.go:348] Generated self-signed cert in-memory
W1208 20:02:47.036894       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1208 20:02:47.036925       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1208 20:02:47.036942       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1208 20:02:47.036957       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1208 20:02:47.046365       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I1208 20:02:47.046382       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 20:02:47.047360       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1208 20:02:47.047378       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1208 20:02:47.047416       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1208 20:02:47.047494       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1208 20:02:47.147770       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1208 21:00:54.826245       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1208 21:00:54.826296       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E1208 21:00:54.826567       1 scheduling_queue.go:963] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1208 21:00:54.826621       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* -- Logs begin at Thu 2022-12-08 21:04:29 UTC, end at Thu 2022-12-08 21:23:45 UTC. --
Dec 08 21:15:32 minikube kubelet[1292]: I1208 21:15:32.175717    1292 scope.go:115] "RemoveContainer" containerID="69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8"
Dec 08 21:15:32 minikube kubelet[1292]: I1208 21:15:32.186130    1292 scope.go:115] "RemoveContainer" containerID="74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251"
Dec 08 21:15:32 minikube kubelet[1292]: E1208 21:15:32.186522    1292 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251" containerID="74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251"
Dec 08 21:15:32 minikube kubelet[1292]: I1208 21:15:32.186547    1292 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251} err="failed to get container status \"74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251\": rpc error: code = Unknown desc = Error: No such container: 74e604650cf58f9ba44895caf4ebe61bec77b9f4cc71ea81a8dc91b5b2ba2251"
Dec 08 21:15:32 minikube kubelet[1292]: I1208 21:15:32.186560    1292 scope.go:115] "RemoveContainer" containerID="69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8"
Dec 08 21:15:32 minikube kubelet[1292]: E1208 21:15:32.186841    1292 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8" containerID="69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8"
Dec 08 21:15:32 minikube kubelet[1292]: I1208 21:15:32.186866    1292 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8} err="failed to get container status \"69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8\": rpc error: code = Unknown desc = Error: No such container: 69b5e057ea565f418661a71aa950b12571729c1fb6ad8f18e97770eb995934f8"
Dec 08 21:15:33 minikube kubelet[1292]: I1208 21:15:33.555805    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=fc52f2e6-9623-4374-aaeb-e7009866147c path="/var/lib/kubelet/pods/fc52f2e6-9623-4374-aaeb-e7009866147c/volumes"
Dec 08 21:18:30 minikube kubelet[1292]: I1208 21:18:30.429553    1292 log.go:198] http: superfluous response.WriteHeader call from github.com/emicklei/go-restful/v3.(*Response).WriteHeader (response.go:221)
Dec 08 21:18:30 minikube kubelet[1292]: I1208 21:18:30.429639    1292 log.go:198] http: superfluous response.WriteHeader call from github.com/emicklei/go-restful/v3.(*Response).WriteHeader (response.go:221)
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.090343    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lpwsq\" (UniqueName: \"kubernetes.io/projected/73ffb853-b7f5-42fc-8597-b475163a9074-kube-api-access-lpwsq\") pod \"73ffb853-b7f5-42fc-8597-b475163a9074\" (UID: \"73ffb853-b7f5-42fc-8597-b475163a9074\") "
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.092222    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/73ffb853-b7f5-42fc-8597-b475163a9074-kube-api-access-lpwsq" (OuterVolumeSpecName: "kube-api-access-lpwsq") pod "73ffb853-b7f5-42fc-8597-b475163a9074" (UID: "73ffb853-b7f5-42fc-8597-b475163a9074"). InnerVolumeSpecName "kube-api-access-lpwsq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.190700    1292 reconciler.go:399] "Volume detached for volume \"kube-api-access-lpwsq\" (UniqueName: \"kubernetes.io/projected/73ffb853-b7f5-42fc-8597-b475163a9074-kube-api-access-lpwsq\") on node \"minikube\" DevicePath \"\""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.202553    1292 scope.go:115] "RemoveContainer" containerID="7de0881b155aee79a4c9d7ebaf3c0da1d085ba6051dc655dac2ab671be94a894"
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.210140    1292 scope.go:115] "RemoveContainer" containerID="0effb39ac182d28946183bca6624002e06592b1c8be2abf1d28f6a603d3f80b5"
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.218456    1292 scope.go:115] "RemoveContainer" containerID="48ca69270ba00b3f13ac4bbde853659f25f1aa311632423eb08de539417b9fb2"
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.290908    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-9h75v\" (UniqueName: \"kubernetes.io/projected/fad66b7f-2cb0-429f-af8c-9daa76e45af7-kube-api-access-9h75v\") pod \"fad66b7f-2cb0-429f-af8c-9daa76e45af7\" (UID: \"fad66b7f-2cb0-429f-af8c-9daa76e45af7\") "
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.290952    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-74tz4\" (UniqueName: \"kubernetes.io/projected/67ee76d7-bacd-484b-963b-24a2f9fd4885-kube-api-access-74tz4\") pod \"67ee76d7-bacd-484b-963b-24a2f9fd4885\" (UID: \"67ee76d7-bacd-484b-963b-24a2f9fd4885\") "
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.292269    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fad66b7f-2cb0-429f-af8c-9daa76e45af7-kube-api-access-9h75v" (OuterVolumeSpecName: "kube-api-access-9h75v") pod "fad66b7f-2cb0-429f-af8c-9daa76e45af7" (UID: "fad66b7f-2cb0-429f-af8c-9daa76e45af7"). InnerVolumeSpecName "kube-api-access-9h75v". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.292269    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/67ee76d7-bacd-484b-963b-24a2f9fd4885-kube-api-access-74tz4" (OuterVolumeSpecName: "kube-api-access-74tz4") pod "67ee76d7-bacd-484b-963b-24a2f9fd4885" (UID: "67ee76d7-bacd-484b-963b-24a2f9fd4885"). InnerVolumeSpecName "kube-api-access-74tz4". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.391763    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pswtp\" (UniqueName: \"kubernetes.io/projected/ae9f8081-07d5-4e16-a75a-55458db560c6-kube-api-access-pswtp\") pod \"ae9f8081-07d5-4e16-a75a-55458db560c6\" (UID: \"ae9f8081-07d5-4e16-a75a-55458db560c6\") "
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.391870    1292 reconciler.go:399] "Volume detached for volume \"kube-api-access-9h75v\" (UniqueName: \"kubernetes.io/projected/fad66b7f-2cb0-429f-af8c-9daa76e45af7-kube-api-access-9h75v\") on node \"minikube\" DevicePath \"\""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.391901    1292 reconciler.go:399] "Volume detached for volume \"kube-api-access-74tz4\" (UniqueName: \"kubernetes.io/projected/67ee76d7-bacd-484b-963b-24a2f9fd4885-kube-api-access-74tz4\") on node \"minikube\" DevicePath \"\""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.394927    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/ae9f8081-07d5-4e16-a75a-55458db560c6-kube-api-access-pswtp" (OuterVolumeSpecName: "kube-api-access-pswtp") pod "ae9f8081-07d5-4e16-a75a-55458db560c6" (UID: "ae9f8081-07d5-4e16-a75a-55458db560c6"). InnerVolumeSpecName "kube-api-access-pswtp". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.492139    1292 reconciler.go:399] "Volume detached for volume \"kube-api-access-pswtp\" (UniqueName: \"kubernetes.io/projected/ae9f8081-07d5-4e16-a75a-55458db560c6-kube-api-access-pswtp\") on node \"minikube\" DevicePath \"\""
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.557605    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=67ee76d7-bacd-484b-963b-24a2f9fd4885 path="/var/lib/kubelet/pods/67ee76d7-bacd-484b-963b-24a2f9fd4885/volumes"
Dec 08 21:18:31 minikube kubelet[1292]: I1208 21:18:31.557790    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=73ffb853-b7f5-42fc-8597-b475163a9074 path="/var/lib/kubelet/pods/73ffb853-b7f5-42fc-8597-b475163a9074/volumes"
Dec 08 21:18:32 minikube kubelet[1292]: I1208 21:18:32.224970    1292 scope.go:115] "RemoveContainer" containerID="6a3ebf39f07a71c603d2d954c86857eec910608456207fb4d840aca358ad5434"
Dec 08 21:18:33 minikube kubelet[1292]: I1208 21:18:33.567428    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=ae9f8081-07d5-4e16-a75a-55458db560c6 path="/var/lib/kubelet/pods/ae9f8081-07d5-4e16-a75a-55458db560c6/volumes"
Dec 08 21:18:33 minikube kubelet[1292]: I1208 21:18:33.567641    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=fad66b7f-2cb0-429f-af8c-9daa76e45af7 path="/var/lib/kubelet/pods/fad66b7f-2cb0-429f-af8c-9daa76e45af7/volumes"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830144    1292 topology_manager.go:205] "Topology Admit Handler"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830224    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="67ee76d7-bacd-484b-963b-24a2f9fd4885" containerName="auth"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830242    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="73ffb853-b7f5-42fc-8597-b475163a9074" containerName="mongo"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830257    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="fc52f2e6-9623-4374-aaeb-e7009866147c" containerName="controller"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830267    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="fad66b7f-2cb0-429f-af8c-9daa76e45af7" containerName="mongo"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830277    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="fc52f2e6-9623-4374-aaeb-e7009866147c" containerName="controller"
Dec 08 21:19:08 minikube kubelet[1292]: E1208 21:19:08.830289    1292 cpu_manager.go:394] "RemoveStaleState: removing container" podUID="ae9f8081-07d5-4e16-a75a-55458db560c6" containerName="tickets"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830335    1292 memory_manager.go:345] "RemoveStaleState removing state" podUID="fc52f2e6-9623-4374-aaeb-e7009866147c" containerName="controller"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830348    1292 memory_manager.go:345] "RemoveStaleState removing state" podUID="67ee76d7-bacd-484b-963b-24a2f9fd4885" containerName="auth"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830359    1292 memory_manager.go:345] "RemoveStaleState removing state" podUID="73ffb853-b7f5-42fc-8597-b475163a9074" containerName="mongo"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830368    1292 memory_manager.go:345] "RemoveStaleState removing state" podUID="fad66b7f-2cb0-429f-af8c-9daa76e45af7" containerName="mongo"
Dec 08 21:19:08 minikube kubelet[1292]: I1208 21:19:08.830381    1292 memory_manager.go:345] "RemoveStaleState removing state" podUID="ae9f8081-07d5-4e16-a75a-55458db560c6" containerName="tickets"
Dec 08 21:19:09 minikube kubelet[1292]: I1208 21:19:09.013397    1292 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/5a046864-bf8e-486e-9d52-9aaeff9e75cc-webhook-cert\") pod \"ingress-nginx-controller-8574b6d7c9-fqnqc\" (UID: \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\") " pod="ingress-nginx/ingress-nginx-controller-8574b6d7c9-fqnqc"
Dec 08 21:19:09 minikube kubelet[1292]: I1208 21:19:09.013435    1292 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zdjv8\" (UniqueName: \"kubernetes.io/projected/5a046864-bf8e-486e-9d52-9aaeff9e75cc-kube-api-access-zdjv8\") pod \"ingress-nginx-controller-8574b6d7c9-fqnqc\" (UID: \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\") " pod="ingress-nginx/ingress-nginx-controller-8574b6d7c9-fqnqc"
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.733102    1292 scope.go:115] "RemoveContainer" containerID="8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.740261    1292 scope.go:115] "RemoveContainer" containerID="8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:24 minikube kubelet[1292]: E1208 21:19:24.740631    1292 remote_runtime.go:599] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea" containerID="8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.740650    1292 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea} err="failed to get container status \"8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea\": rpc error: code = Unknown desc = Error: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.916704    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/5a046864-bf8e-486e-9d52-9aaeff9e75cc-webhook-cert\") pod \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\" (UID: \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\") "
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.916801    1292 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zdjv8\" (UniqueName: \"kubernetes.io/projected/5a046864-bf8e-486e-9d52-9aaeff9e75cc-kube-api-access-zdjv8\") pod \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\" (UID: \"5a046864-bf8e-486e-9d52-9aaeff9e75cc\") "
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.919719    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/5a046864-bf8e-486e-9d52-9aaeff9e75cc-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "5a046864-bf8e-486e-9d52-9aaeff9e75cc" (UID: "5a046864-bf8e-486e-9d52-9aaeff9e75cc"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Dec 08 21:19:24 minikube kubelet[1292]: I1208 21:19:24.919995    1292 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/5a046864-bf8e-486e-9d52-9aaeff9e75cc-kube-api-access-zdjv8" (OuterVolumeSpecName: "kube-api-access-zdjv8") pod "5a046864-bf8e-486e-9d52-9aaeff9e75cc" (UID: "5a046864-bf8e-486e-9d52-9aaeff9e75cc"). InnerVolumeSpecName "kube-api-access-zdjv8". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 08 21:19:25 minikube kubelet[1292]: I1208 21:19:25.017578    1292 reconciler.go:399] "Volume detached for volume \"kube-api-access-zdjv8\" (UniqueName: \"kubernetes.io/projected/5a046864-bf8e-486e-9d52-9aaeff9e75cc-kube-api-access-zdjv8\") on node \"minikube\" DevicePath \"\""
Dec 08 21:19:25 minikube kubelet[1292]: I1208 21:19:25.017632    1292 reconciler.go:399] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/5a046864-bf8e-486e-9d52-9aaeff9e75cc-webhook-cert\") on node \"minikube\" DevicePath \"\""
Dec 08 21:19:25 minikube kubelet[1292]: E1208 21:19:25.552515    1292 remote_runtime.go:505] "StopContainer from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea" containerID="8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:25 minikube kubelet[1292]: E1208 21:19:25.552632    1292 kuberuntime_container.go:707] "Container termination failed with gracePeriod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea" pod="ingress-nginx/ingress-nginx-controller-8574b6d7c9-fqnqc" podUID=5a046864-bf8e-486e-9d52-9aaeff9e75cc containerName="controller" containerID="docker://8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea" gracePeriod=1
Dec 08 21:19:25 minikube kubelet[1292]: E1208 21:19:25.552688    1292 kuberuntime_container.go:732] "Kill container failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea" pod="ingress-nginx/ingress-nginx-controller-8574b6d7c9-fqnqc" podUID=5a046864-bf8e-486e-9d52-9aaeff9e75cc containerName="controller" containerID={Type:docker ID:8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea}
Dec 08 21:19:25 minikube kubelet[1292]: E1208 21:19:25.553780    1292 kubelet.go:1781] failed to "KillContainer" for "controller" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea"
Dec 08 21:19:25 minikube kubelet[1292]: E1208 21:19:25.553804    1292 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 8e41c8256cdab0615b157872d042d3c8c27d61edd825f70a2db41849e9b4a4ea\"" pod="ingress-nginx/ingress-nginx-controller-8574b6d7c9-fqnqc" podUID=5a046864-bf8e-486e-9d52-9aaeff9e75cc
Dec 08 21:19:25 minikube kubelet[1292]: I1208 21:19:25.559233    1292 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=5a046864-bf8e-486e-9d52-9aaeff9e75cc path="/var/lib/kubelet/pods/5a046864-bf8e-486e-9d52-9aaeff9e75cc/volumes"

* 
* ==> storage-provisioner [307aba3820ea] <==
* I1208 21:05:11.691445       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1208 21:05:11.701261       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1208 21:05:11.701569       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1208 21:05:29.097725       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1208 21:05:29.097836       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"94bc18d7-c9d6-4b52-84fc-bac490d6153d", APIVersion:"v1", ResourceVersion:"17682", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8cc04578-1814-4aef-9bce-78bfd1221eb9 became leader
I1208 21:05:29.097901       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8cc04578-1814-4aef-9bce-78bfd1221eb9!
I1208 21:05:29.198653       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8cc04578-1814-4aef-9bce-78bfd1221eb9!

* 
* ==> storage-provisioner [5030bf98d40b] <==
* I1208 21:04:45.052896       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1208 21:04:55.062221       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

